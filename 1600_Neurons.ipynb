{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab8b8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\envs\\registration.py:592: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "gym.new_step_api=True\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "H = 1600 # number of hidden layer neurons\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "# hyperparameters\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  I=np.asarray(I)\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  \n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  \n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  \n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(epx, eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "def model_step(model, observation, prev_x):\n",
    "\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "  \n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, _ = policy_forward(x)\n",
    "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
    "  \n",
    "  return action, prev_x\n",
    "\n",
    "def play_game(env, model):\n",
    "  observation = env.reset()\n",
    "\n",
    "  frames = []\n",
    "  cumulated_reward = 0\n",
    "\n",
    "  prev_x = None # used in computing the difference frame\n",
    "\n",
    "  for t in range(1000):\n",
    "      frames.append(env.render(mode = 'rgb_array'))\n",
    "      action, prev_x = model_step(model, observation, prev_x)\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      cumulated_reward += reward\n",
    "      if done:\n",
    "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
    "          break\n",
    "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
    "  env.close()\n",
    "  display_frames_as_gif(frames)\n",
    "\n",
    "def train_model(env, model, total_episodes = 100):\n",
    "  hist = []\n",
    "  observation = env.reset()\n",
    "\n",
    "  prev_x = None # used in computing the difference frame\n",
    "  xs,hs,dlogps,drs = [],[],[],[]\n",
    "  running_reward = None\n",
    "  reward_sum = 0\n",
    "  episode_number = 0\n",
    "\n",
    "  while True:\n",
    "  \n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "      episode_number += 1\n",
    "\n",
    "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "      epx = np.vstack(xs)\n",
    "      eph = np.vstack(hs)\n",
    "      epdlogp = np.vstack(dlogps)\n",
    "      epr = np.vstack(drs)\n",
    "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "      # compute the discounted reward backwards through time\n",
    "      discounted_epr = discount_rewards(epr)\n",
    "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "      discounted_epr -= np.mean(discounted_epr)\n",
    "      discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "      grad = policy_backward(epx, eph, epdlogp)\n",
    "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "      # perform rmsprop parameter update every batch_size episodes\n",
    "      if episode_number % batch_size == 0:\n",
    "        for k,v in model.items():\n",
    "          g = grad_buffer[k] # gradient\n",
    "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "      # boring book-keeping\n",
    "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "      hist.append((episode_number, reward_sum, running_reward))\n",
    "      print ('episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
    "      reward_sum = 0\n",
    "      observation = env.reset() # reset env\n",
    "      prev_x = None\n",
    "      if episode_number == total_episodes: \n",
    "        return hist\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed67bf8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
      "episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
      "episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
      "episode 4.000000, reward total was -20.000000. running mean: -20.980199\n",
      "episode 5.000000, reward total was -21.000000. running mean: -20.980397\n",
      "episode 6.000000, reward total was -21.000000. running mean: -20.980593\n",
      "episode 7.000000, reward total was -19.000000. running mean: -20.960787\n",
      "episode 8.000000, reward total was -21.000000. running mean: -20.961179\n",
      "episode 9.000000, reward total was -18.000000. running mean: -20.931567\n",
      "episode 10.000000, reward total was -21.000000. running mean: -20.932252\n",
      "episode 11.000000, reward total was -21.000000. running mean: -20.932929\n",
      "episode 12.000000, reward total was -20.000000. running mean: -20.923600\n",
      "episode 13.000000, reward total was -19.000000. running mean: -20.904364\n",
      "episode 14.000000, reward total was -17.000000. running mean: -20.865320\n",
      "episode 15.000000, reward total was -20.000000. running mean: -20.856667\n",
      "episode 16.000000, reward total was -21.000000. running mean: -20.858100\n",
      "episode 17.000000, reward total was -20.000000. running mean: -20.849519\n",
      "episode 18.000000, reward total was -21.000000. running mean: -20.851024\n",
      "episode 19.000000, reward total was -21.000000. running mean: -20.852514\n",
      "episode 20.000000, reward total was -21.000000. running mean: -20.853989\n",
      "episode 21.000000, reward total was -20.000000. running mean: -20.845449\n",
      "episode 22.000000, reward total was -21.000000. running mean: -20.846994\n",
      "episode 23.000000, reward total was -20.000000. running mean: -20.838525\n",
      "episode 24.000000, reward total was -21.000000. running mean: -20.840139\n",
      "episode 25.000000, reward total was -20.000000. running mean: -20.831738\n",
      "episode 26.000000, reward total was -21.000000. running mean: -20.833421\n",
      "episode 27.000000, reward total was -21.000000. running mean: -20.835086\n",
      "episode 28.000000, reward total was -21.000000. running mean: -20.836735\n",
      "episode 29.000000, reward total was -20.000000. running mean: -20.828368\n",
      "episode 30.000000, reward total was -19.000000. running mean: -20.810084\n",
      "episode 31.000000, reward total was -21.000000. running mean: -20.811984\n",
      "episode 32.000000, reward total was -21.000000. running mean: -20.813864\n",
      "episode 33.000000, reward total was -18.000000. running mean: -20.785725\n",
      "episode 34.000000, reward total was -20.000000. running mean: -20.777868\n",
      "episode 35.000000, reward total was -20.000000. running mean: -20.770089\n",
      "episode 36.000000, reward total was -21.000000. running mean: -20.772388\n",
      "episode 37.000000, reward total was -20.000000. running mean: -20.764664\n",
      "episode 38.000000, reward total was -20.000000. running mean: -20.757018\n",
      "episode 39.000000, reward total was -19.000000. running mean: -20.739448\n",
      "episode 40.000000, reward total was -20.000000. running mean: -20.732053\n",
      "episode 41.000000, reward total was -18.000000. running mean: -20.704733\n",
      "episode 42.000000, reward total was -20.000000. running mean: -20.697685\n",
      "episode 43.000000, reward total was -20.000000. running mean: -20.690708\n",
      "episode 44.000000, reward total was -21.000000. running mean: -20.693801\n",
      "episode 45.000000, reward total was -20.000000. running mean: -20.686863\n",
      "episode 46.000000, reward total was -20.000000. running mean: -20.679995\n",
      "episode 47.000000, reward total was -21.000000. running mean: -20.683195\n",
      "episode 48.000000, reward total was -21.000000. running mean: -20.686363\n",
      "episode 49.000000, reward total was -21.000000. running mean: -20.689499\n",
      "episode 50.000000, reward total was -19.000000. running mean: -20.672604\n",
      "episode 51.000000, reward total was -21.000000. running mean: -20.675878\n",
      "episode 52.000000, reward total was -21.000000. running mean: -20.679119\n",
      "episode 53.000000, reward total was -20.000000. running mean: -20.672328\n",
      "episode 54.000000, reward total was -20.000000. running mean: -20.665605\n",
      "episode 55.000000, reward total was -21.000000. running mean: -20.668949\n",
      "episode 56.000000, reward total was -21.000000. running mean: -20.672259\n",
      "episode 57.000000, reward total was -21.000000. running mean: -20.675537\n",
      "episode 58.000000, reward total was -21.000000. running mean: -20.678781\n",
      "episode 59.000000, reward total was -21.000000. running mean: -20.681994\n",
      "episode 60.000000, reward total was -21.000000. running mean: -20.685174\n",
      "episode 61.000000, reward total was -21.000000. running mean: -20.688322\n",
      "episode 62.000000, reward total was -20.000000. running mean: -20.681439\n",
      "episode 63.000000, reward total was -20.000000. running mean: -20.674624\n",
      "episode 64.000000, reward total was -19.000000. running mean: -20.657878\n",
      "episode 65.000000, reward total was -21.000000. running mean: -20.661299\n",
      "episode 66.000000, reward total was -19.000000. running mean: -20.644686\n",
      "episode 67.000000, reward total was -19.000000. running mean: -20.628239\n",
      "episode 68.000000, reward total was -20.000000. running mean: -20.621957\n",
      "episode 69.000000, reward total was -21.000000. running mean: -20.625737\n",
      "episode 70.000000, reward total was -20.000000. running mean: -20.619480\n",
      "episode 71.000000, reward total was -20.000000. running mean: -20.613285\n",
      "episode 72.000000, reward total was -20.000000. running mean: -20.607152\n",
      "episode 73.000000, reward total was -21.000000. running mean: -20.611081\n",
      "episode 74.000000, reward total was -20.000000. running mean: -20.604970\n",
      "episode 75.000000, reward total was -21.000000. running mean: -20.608920\n",
      "episode 76.000000, reward total was -21.000000. running mean: -20.612831\n",
      "episode 77.000000, reward total was -20.000000. running mean: -20.606703\n",
      "episode 78.000000, reward total was -21.000000. running mean: -20.610636\n",
      "episode 79.000000, reward total was -20.000000. running mean: -20.604529\n",
      "episode 80.000000, reward total was -21.000000. running mean: -20.608484\n",
      "episode 81.000000, reward total was -21.000000. running mean: -20.612399\n",
      "episode 82.000000, reward total was -18.000000. running mean: -20.586275\n",
      "episode 83.000000, reward total was -21.000000. running mean: -20.590413\n",
      "episode 84.000000, reward total was -21.000000. running mean: -20.594508\n",
      "episode 85.000000, reward total was -21.000000. running mean: -20.598563\n",
      "episode 86.000000, reward total was -21.000000. running mean: -20.602578\n",
      "episode 87.000000, reward total was -21.000000. running mean: -20.606552\n",
      "episode 88.000000, reward total was -20.000000. running mean: -20.600486\n",
      "episode 89.000000, reward total was -21.000000. running mean: -20.604482\n",
      "episode 90.000000, reward total was -20.000000. running mean: -20.598437\n",
      "episode 91.000000, reward total was -21.000000. running mean: -20.602452\n",
      "episode 92.000000, reward total was -21.000000. running mean: -20.606428\n",
      "episode 93.000000, reward total was -21.000000. running mean: -20.610364\n",
      "episode 94.000000, reward total was -21.000000. running mean: -20.614260\n",
      "episode 95.000000, reward total was -21.000000. running mean: -20.618117\n",
      "episode 96.000000, reward total was -20.000000. running mean: -20.611936\n",
      "episode 97.000000, reward total was -20.000000. running mean: -20.605817\n",
      "episode 98.000000, reward total was -20.000000. running mean: -20.599759\n",
      "episode 99.000000, reward total was -19.000000. running mean: -20.583761\n",
      "episode 100.000000, reward total was -20.000000. running mean: -20.577923\n",
      "episode 101.000000, reward total was -21.000000. running mean: -20.582144\n",
      "episode 102.000000, reward total was -20.000000. running mean: -20.576323\n",
      "episode 103.000000, reward total was -21.000000. running mean: -20.580560\n",
      "episode 104.000000, reward total was -20.000000. running mean: -20.574754\n",
      "episode 105.000000, reward total was -21.000000. running mean: -20.579006\n",
      "episode 106.000000, reward total was -19.000000. running mean: -20.563216\n",
      "episode 107.000000, reward total was -20.000000. running mean: -20.557584\n",
      "episode 108.000000, reward total was -21.000000. running mean: -20.562008\n",
      "episode 109.000000, reward total was -20.000000. running mean: -20.556388\n",
      "episode 110.000000, reward total was -20.000000. running mean: -20.550824\n",
      "episode 111.000000, reward total was -21.000000. running mean: -20.555316\n",
      "episode 112.000000, reward total was -21.000000. running mean: -20.559763\n",
      "episode 113.000000, reward total was -20.000000. running mean: -20.554165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 114.000000, reward total was -21.000000. running mean: -20.558624\n",
      "episode 115.000000, reward total was -20.000000. running mean: -20.553037\n",
      "episode 116.000000, reward total was -20.000000. running mean: -20.547507\n",
      "episode 117.000000, reward total was -19.000000. running mean: -20.532032\n",
      "episode 118.000000, reward total was -21.000000. running mean: -20.536712\n",
      "episode 119.000000, reward total was -20.000000. running mean: -20.531345\n",
      "episode 120.000000, reward total was -21.000000. running mean: -20.536031\n",
      "episode 121.000000, reward total was -20.000000. running mean: -20.530671\n",
      "episode 122.000000, reward total was -21.000000. running mean: -20.535364\n",
      "episode 123.000000, reward total was -21.000000. running mean: -20.540010\n",
      "episode 124.000000, reward total was -20.000000. running mean: -20.534610\n",
      "episode 125.000000, reward total was -20.000000. running mean: -20.529264\n",
      "episode 126.000000, reward total was -21.000000. running mean: -20.533972\n",
      "episode 127.000000, reward total was -20.000000. running mean: -20.528632\n",
      "episode 128.000000, reward total was -21.000000. running mean: -20.533346\n",
      "episode 129.000000, reward total was -20.000000. running mean: -20.528012\n",
      "episode 130.000000, reward total was -20.000000. running mean: -20.522732\n",
      "episode 131.000000, reward total was -20.000000. running mean: -20.517505\n",
      "episode 132.000000, reward total was -21.000000. running mean: -20.522330\n",
      "episode 133.000000, reward total was -19.000000. running mean: -20.507106\n",
      "episode 134.000000, reward total was -19.000000. running mean: -20.492035\n",
      "episode 135.000000, reward total was -21.000000. running mean: -20.497115\n",
      "episode 136.000000, reward total was -21.000000. running mean: -20.502144\n",
      "episode 137.000000, reward total was -21.000000. running mean: -20.507122\n",
      "episode 138.000000, reward total was -20.000000. running mean: -20.502051\n",
      "episode 139.000000, reward total was -21.000000. running mean: -20.507031\n",
      "episode 140.000000, reward total was -20.000000. running mean: -20.501960\n",
      "episode 141.000000, reward total was -21.000000. running mean: -20.506941\n",
      "episode 142.000000, reward total was -20.000000. running mean: -20.501871\n",
      "episode 143.000000, reward total was -21.000000. running mean: -20.506853\n",
      "episode 144.000000, reward total was -21.000000. running mean: -20.511784\n",
      "episode 145.000000, reward total was -20.000000. running mean: -20.506666\n",
      "episode 146.000000, reward total was -21.000000. running mean: -20.511600\n",
      "episode 147.000000, reward total was -21.000000. running mean: -20.516484\n",
      "episode 148.000000, reward total was -21.000000. running mean: -20.521319\n",
      "episode 149.000000, reward total was -21.000000. running mean: -20.526106\n",
      "episode 150.000000, reward total was -19.000000. running mean: -20.510844\n",
      "episode 151.000000, reward total was -21.000000. running mean: -20.515736\n",
      "episode 152.000000, reward total was -20.000000. running mean: -20.510579\n",
      "episode 153.000000, reward total was -21.000000. running mean: -20.515473\n",
      "episode 154.000000, reward total was -21.000000. running mean: -20.520318\n",
      "episode 155.000000, reward total was -20.000000. running mean: -20.515115\n",
      "episode 156.000000, reward total was -21.000000. running mean: -20.519964\n",
      "episode 157.000000, reward total was -20.000000. running mean: -20.514764\n",
      "episode 158.000000, reward total was -20.000000. running mean: -20.509617\n",
      "episode 159.000000, reward total was -19.000000. running mean: -20.494520\n",
      "episode 160.000000, reward total was -21.000000. running mean: -20.499575\n",
      "episode 161.000000, reward total was -21.000000. running mean: -20.504579\n",
      "episode 162.000000, reward total was -21.000000. running mean: -20.509534\n",
      "episode 163.000000, reward total was -18.000000. running mean: -20.484438\n",
      "episode 164.000000, reward total was -20.000000. running mean: -20.479594\n",
      "episode 165.000000, reward total was -21.000000. running mean: -20.484798\n",
      "episode 166.000000, reward total was -21.000000. running mean: -20.489950\n",
      "episode 167.000000, reward total was -20.000000. running mean: -20.485050\n",
      "episode 168.000000, reward total was -21.000000. running mean: -20.490200\n",
      "episode 169.000000, reward total was -20.000000. running mean: -20.485298\n",
      "episode 170.000000, reward total was -21.000000. running mean: -20.490445\n",
      "episode 171.000000, reward total was -20.000000. running mean: -20.485541\n",
      "episode 172.000000, reward total was -21.000000. running mean: -20.490685\n",
      "episode 173.000000, reward total was -18.000000. running mean: -20.465778\n",
      "episode 174.000000, reward total was -18.000000. running mean: -20.441121\n",
      "episode 175.000000, reward total was -21.000000. running mean: -20.446709\n",
      "episode 176.000000, reward total was -20.000000. running mean: -20.442242\n",
      "episode 177.000000, reward total was -21.000000. running mean: -20.447820\n",
      "episode 178.000000, reward total was -19.000000. running mean: -20.433342\n",
      "episode 179.000000, reward total was -20.000000. running mean: -20.429008\n",
      "episode 180.000000, reward total was -21.000000. running mean: -20.434718\n",
      "episode 181.000000, reward total was -19.000000. running mean: -20.420371\n",
      "episode 182.000000, reward total was -19.000000. running mean: -20.406167\n",
      "episode 183.000000, reward total was -20.000000. running mean: -20.402106\n",
      "episode 184.000000, reward total was -21.000000. running mean: -20.408084\n",
      "episode 185.000000, reward total was -20.000000. running mean: -20.404004\n",
      "episode 186.000000, reward total was -21.000000. running mean: -20.409964\n",
      "episode 187.000000, reward total was -18.000000. running mean: -20.385864\n",
      "episode 188.000000, reward total was -20.000000. running mean: -20.382005\n",
      "episode 189.000000, reward total was -20.000000. running mean: -20.378185\n",
      "episode 190.000000, reward total was -20.000000. running mean: -20.374403\n",
      "episode 191.000000, reward total was -21.000000. running mean: -20.380659\n",
      "episode 192.000000, reward total was -21.000000. running mean: -20.386853\n",
      "episode 193.000000, reward total was -21.000000. running mean: -20.392984\n",
      "episode 194.000000, reward total was -20.000000. running mean: -20.389054\n",
      "episode 195.000000, reward total was -21.000000. running mean: -20.395164\n",
      "episode 196.000000, reward total was -20.000000. running mean: -20.391212\n",
      "episode 197.000000, reward total was -20.000000. running mean: -20.387300\n",
      "episode 198.000000, reward total was -21.000000. running mean: -20.393427\n",
      "episode 199.000000, reward total was -19.000000. running mean: -20.379493\n",
      "episode 200.000000, reward total was -19.000000. running mean: -20.365698\n",
      "episode 201.000000, reward total was -20.000000. running mean: -20.362041\n",
      "episode 202.000000, reward total was -19.000000. running mean: -20.348421\n",
      "episode 203.000000, reward total was -21.000000. running mean: -20.354936\n",
      "episode 204.000000, reward total was -20.000000. running mean: -20.351387\n",
      "episode 205.000000, reward total was -21.000000. running mean: -20.357873\n",
      "episode 206.000000, reward total was -19.000000. running mean: -20.344294\n",
      "episode 207.000000, reward total was -20.000000. running mean: -20.340851\n",
      "episode 208.000000, reward total was -20.000000. running mean: -20.337443\n",
      "episode 209.000000, reward total was -21.000000. running mean: -20.344068\n",
      "episode 210.000000, reward total was -21.000000. running mean: -20.350628\n",
      "episode 211.000000, reward total was -19.000000. running mean: -20.337122\n",
      "episode 212.000000, reward total was -21.000000. running mean: -20.343750\n",
      "episode 213.000000, reward total was -21.000000. running mean: -20.350313\n",
      "episode 214.000000, reward total was -20.000000. running mean: -20.346810\n",
      "episode 215.000000, reward total was -19.000000. running mean: -20.333342\n",
      "episode 216.000000, reward total was -18.000000. running mean: -20.310008\n",
      "episode 217.000000, reward total was -20.000000. running mean: -20.306908\n",
      "episode 218.000000, reward total was -20.000000. running mean: -20.303839\n",
      "episode 219.000000, reward total was -21.000000. running mean: -20.310801\n",
      "episode 220.000000, reward total was -21.000000. running mean: -20.317693\n",
      "episode 221.000000, reward total was -21.000000. running mean: -20.324516\n",
      "episode 222.000000, reward total was -20.000000. running mean: -20.321271\n",
      "episode 223.000000, reward total was -20.000000. running mean: -20.318058\n",
      "episode 224.000000, reward total was -19.000000. running mean: -20.304877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 225.000000, reward total was -20.000000. running mean: -20.301828\n",
      "episode 226.000000, reward total was -19.000000. running mean: -20.288810\n",
      "episode 227.000000, reward total was -20.000000. running mean: -20.285922\n",
      "episode 228.000000, reward total was -21.000000. running mean: -20.293063\n",
      "episode 229.000000, reward total was -20.000000. running mean: -20.290132\n",
      "episode 230.000000, reward total was -21.000000. running mean: -20.297231\n",
      "episode 231.000000, reward total was -20.000000. running mean: -20.294259\n",
      "episode 232.000000, reward total was -20.000000. running mean: -20.291316\n",
      "episode 233.000000, reward total was -21.000000. running mean: -20.298403\n",
      "episode 234.000000, reward total was -21.000000. running mean: -20.305419\n",
      "episode 235.000000, reward total was -21.000000. running mean: -20.312365\n",
      "episode 236.000000, reward total was -19.000000. running mean: -20.299241\n",
      "episode 237.000000, reward total was -19.000000. running mean: -20.286249\n",
      "episode 238.000000, reward total was -21.000000. running mean: -20.293386\n",
      "episode 239.000000, reward total was -21.000000. running mean: -20.300452\n",
      "episode 240.000000, reward total was -20.000000. running mean: -20.297448\n",
      "episode 241.000000, reward total was -21.000000. running mean: -20.304473\n",
      "episode 242.000000, reward total was -20.000000. running mean: -20.301428\n",
      "episode 243.000000, reward total was -21.000000. running mean: -20.308414\n",
      "episode 244.000000, reward total was -20.000000. running mean: -20.305330\n",
      "episode 245.000000, reward total was -21.000000. running mean: -20.312277\n",
      "episode 246.000000, reward total was -18.000000. running mean: -20.289154\n",
      "episode 247.000000, reward total was -21.000000. running mean: -20.296262\n",
      "episode 248.000000, reward total was -20.000000. running mean: -20.293300\n",
      "episode 249.000000, reward total was -20.000000. running mean: -20.290367\n",
      "episode 250.000000, reward total was -20.000000. running mean: -20.287463\n",
      "episode 251.000000, reward total was -19.000000. running mean: -20.274589\n",
      "episode 252.000000, reward total was -21.000000. running mean: -20.281843\n",
      "episode 253.000000, reward total was -20.000000. running mean: -20.279024\n",
      "episode 254.000000, reward total was -21.000000. running mean: -20.286234\n",
      "episode 255.000000, reward total was -21.000000. running mean: -20.293372\n",
      "episode 256.000000, reward total was -20.000000. running mean: -20.290438\n",
      "episode 257.000000, reward total was -20.000000. running mean: -20.287534\n",
      "episode 258.000000, reward total was -21.000000. running mean: -20.294658\n",
      "episode 259.000000, reward total was -21.000000. running mean: -20.301712\n",
      "episode 260.000000, reward total was -21.000000. running mean: -20.308695\n",
      "episode 261.000000, reward total was -21.000000. running mean: -20.315608\n",
      "episode 262.000000, reward total was -21.000000. running mean: -20.322451\n",
      "episode 263.000000, reward total was -21.000000. running mean: -20.329227\n",
      "episode 264.000000, reward total was -21.000000. running mean: -20.335935\n",
      "episode 265.000000, reward total was -21.000000. running mean: -20.342575\n",
      "episode 266.000000, reward total was -20.000000. running mean: -20.339150\n",
      "episode 267.000000, reward total was -21.000000. running mean: -20.345758\n",
      "episode 268.000000, reward total was -21.000000. running mean: -20.352301\n",
      "episode 269.000000, reward total was -20.000000. running mean: -20.348778\n",
      "episode 270.000000, reward total was -21.000000. running mean: -20.355290\n",
      "episode 271.000000, reward total was -20.000000. running mean: -20.351737\n",
      "episode 272.000000, reward total was -21.000000. running mean: -20.358219\n",
      "episode 273.000000, reward total was -21.000000. running mean: -20.364637\n",
      "episode 274.000000, reward total was -20.000000. running mean: -20.360991\n",
      "episode 275.000000, reward total was -21.000000. running mean: -20.367381\n",
      "episode 276.000000, reward total was -21.000000. running mean: -20.373707\n",
      "episode 277.000000, reward total was -21.000000. running mean: -20.379970\n",
      "episode 278.000000, reward total was -19.000000. running mean: -20.366170\n",
      "episode 279.000000, reward total was -21.000000. running mean: -20.372509\n",
      "episode 280.000000, reward total was -20.000000. running mean: -20.368784\n",
      "episode 281.000000, reward total was -20.000000. running mean: -20.365096\n",
      "episode 282.000000, reward total was -20.000000. running mean: -20.361445\n",
      "episode 283.000000, reward total was -18.000000. running mean: -20.337830\n",
      "episode 284.000000, reward total was -21.000000. running mean: -20.344452\n",
      "episode 285.000000, reward total was -20.000000. running mean: -20.341008\n",
      "episode 286.000000, reward total was -21.000000. running mean: -20.347597\n",
      "episode 287.000000, reward total was -20.000000. running mean: -20.344122\n",
      "episode 288.000000, reward total was -21.000000. running mean: -20.350680\n",
      "episode 289.000000, reward total was -19.000000. running mean: -20.337173\n",
      "episode 290.000000, reward total was -21.000000. running mean: -20.343802\n",
      "episode 291.000000, reward total was -21.000000. running mean: -20.350364\n",
      "episode 292.000000, reward total was -20.000000. running mean: -20.346860\n",
      "episode 293.000000, reward total was -20.000000. running mean: -20.343392\n",
      "episode 294.000000, reward total was -20.000000. running mean: -20.339958\n",
      "episode 295.000000, reward total was -21.000000. running mean: -20.346558\n",
      "episode 296.000000, reward total was -21.000000. running mean: -20.353092\n",
      "episode 297.000000, reward total was -21.000000. running mean: -20.359562\n",
      "episode 298.000000, reward total was -20.000000. running mean: -20.355966\n",
      "episode 299.000000, reward total was -21.000000. running mean: -20.362406\n",
      "episode 300.000000, reward total was -21.000000. running mean: -20.368782\n",
      "episode 301.000000, reward total was -20.000000. running mean: -20.365094\n",
      "episode 302.000000, reward total was -20.000000. running mean: -20.361443\n",
      "episode 303.000000, reward total was -20.000000. running mean: -20.357829\n",
      "episode 304.000000, reward total was -19.000000. running mean: -20.344251\n",
      "episode 305.000000, reward total was -21.000000. running mean: -20.350808\n",
      "episode 306.000000, reward total was -21.000000. running mean: -20.357300\n",
      "episode 307.000000, reward total was -19.000000. running mean: -20.343727\n",
      "episode 308.000000, reward total was -21.000000. running mean: -20.350290\n",
      "episode 309.000000, reward total was -20.000000. running mean: -20.346787\n",
      "episode 310.000000, reward total was -20.000000. running mean: -20.343319\n",
      "episode 311.000000, reward total was -21.000000. running mean: -20.349886\n",
      "episode 312.000000, reward total was -21.000000. running mean: -20.356387\n",
      "episode 313.000000, reward total was -20.000000. running mean: -20.352823\n",
      "episode 314.000000, reward total was -21.000000. running mean: -20.359295\n",
      "episode 315.000000, reward total was -19.000000. running mean: -20.345702\n",
      "episode 316.000000, reward total was -18.000000. running mean: -20.322245\n",
      "episode 317.000000, reward total was -19.000000. running mean: -20.309022\n",
      "episode 318.000000, reward total was -20.000000. running mean: -20.305932\n",
      "episode 319.000000, reward total was -20.000000. running mean: -20.302873\n",
      "episode 320.000000, reward total was -21.000000. running mean: -20.309844\n",
      "episode 321.000000, reward total was -20.000000. running mean: -20.306746\n",
      "episode 322.000000, reward total was -18.000000. running mean: -20.283678\n",
      "episode 323.000000, reward total was -21.000000. running mean: -20.290842\n",
      "episode 324.000000, reward total was -20.000000. running mean: -20.287933\n",
      "episode 325.000000, reward total was -21.000000. running mean: -20.295054\n",
      "episode 326.000000, reward total was -20.000000. running mean: -20.292103\n",
      "episode 327.000000, reward total was -21.000000. running mean: -20.299182\n",
      "episode 328.000000, reward total was -21.000000. running mean: -20.306190\n",
      "episode 329.000000, reward total was -21.000000. running mean: -20.313128\n",
      "episode 330.000000, reward total was -18.000000. running mean: -20.289997\n",
      "episode 331.000000, reward total was -19.000000. running mean: -20.277097\n",
      "episode 332.000000, reward total was -20.000000. running mean: -20.274326\n",
      "episode 333.000000, reward total was -21.000000. running mean: -20.281583\n",
      "episode 334.000000, reward total was -20.000000. running mean: -20.278767\n",
      "episode 335.000000, reward total was -19.000000. running mean: -20.265979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 336.000000, reward total was -20.000000. running mean: -20.263320\n",
      "episode 337.000000, reward total was -21.000000. running mean: -20.270686\n",
      "episode 338.000000, reward total was -19.000000. running mean: -20.257980\n",
      "episode 339.000000, reward total was -20.000000. running mean: -20.255400\n",
      "episode 340.000000, reward total was -18.000000. running mean: -20.232846\n",
      "episode 341.000000, reward total was -20.000000. running mean: -20.230517\n",
      "episode 342.000000, reward total was -18.000000. running mean: -20.208212\n",
      "episode 343.000000, reward total was -20.000000. running mean: -20.206130\n",
      "episode 344.000000, reward total was -19.000000. running mean: -20.194069\n",
      "episode 345.000000, reward total was -21.000000. running mean: -20.202128\n",
      "episode 346.000000, reward total was -21.000000. running mean: -20.210107\n",
      "episode 347.000000, reward total was -19.000000. running mean: -20.198006\n",
      "episode 348.000000, reward total was -21.000000. running mean: -20.206026\n",
      "episode 349.000000, reward total was -19.000000. running mean: -20.193965\n",
      "episode 350.000000, reward total was -21.000000. running mean: -20.202026\n",
      "episode 351.000000, reward total was -21.000000. running mean: -20.210006\n",
      "episode 352.000000, reward total was -21.000000. running mean: -20.217905\n",
      "episode 353.000000, reward total was -20.000000. running mean: -20.215726\n",
      "episode 354.000000, reward total was -21.000000. running mean: -20.223569\n",
      "episode 355.000000, reward total was -21.000000. running mean: -20.231333\n",
      "episode 356.000000, reward total was -19.000000. running mean: -20.219020\n",
      "episode 357.000000, reward total was -21.000000. running mean: -20.226830\n",
      "episode 358.000000, reward total was -21.000000. running mean: -20.234562\n",
      "episode 359.000000, reward total was -19.000000. running mean: -20.222216\n",
      "episode 360.000000, reward total was -20.000000. running mean: -20.219994\n",
      "episode 361.000000, reward total was -17.000000. running mean: -20.187794\n",
      "episode 362.000000, reward total was -18.000000. running mean: -20.165916\n",
      "episode 363.000000, reward total was -21.000000. running mean: -20.174257\n",
      "episode 364.000000, reward total was -21.000000. running mean: -20.182514\n",
      "episode 365.000000, reward total was -20.000000. running mean: -20.180689\n",
      "episode 366.000000, reward total was -21.000000. running mean: -20.188882\n",
      "episode 367.000000, reward total was -20.000000. running mean: -20.186993\n",
      "episode 368.000000, reward total was -20.000000. running mean: -20.185123\n",
      "episode 369.000000, reward total was -21.000000. running mean: -20.193272\n",
      "episode 370.000000, reward total was -17.000000. running mean: -20.161339\n",
      "episode 371.000000, reward total was -20.000000. running mean: -20.159726\n",
      "episode 372.000000, reward total was -21.000000. running mean: -20.168129\n",
      "episode 373.000000, reward total was -19.000000. running mean: -20.156448\n",
      "episode 374.000000, reward total was -21.000000. running mean: -20.164883\n",
      "episode 375.000000, reward total was -20.000000. running mean: -20.163234\n",
      "episode 376.000000, reward total was -18.000000. running mean: -20.141602\n",
      "episode 377.000000, reward total was -21.000000. running mean: -20.150186\n",
      "episode 378.000000, reward total was -21.000000. running mean: -20.158684\n",
      "episode 379.000000, reward total was -20.000000. running mean: -20.157097\n",
      "episode 380.000000, reward total was -21.000000. running mean: -20.165526\n",
      "episode 381.000000, reward total was -21.000000. running mean: -20.173871\n",
      "episode 382.000000, reward total was -21.000000. running mean: -20.182132\n",
      "episode 383.000000, reward total was -21.000000. running mean: -20.190311\n",
      "episode 384.000000, reward total was -21.000000. running mean: -20.198408\n",
      "episode 385.000000, reward total was -20.000000. running mean: -20.196424\n",
      "episode 386.000000, reward total was -20.000000. running mean: -20.194459\n",
      "episode 387.000000, reward total was -21.000000. running mean: -20.202515\n",
      "episode 388.000000, reward total was -19.000000. running mean: -20.190490\n",
      "episode 389.000000, reward total was -20.000000. running mean: -20.188585\n",
      "episode 390.000000, reward total was -21.000000. running mean: -20.196699\n",
      "episode 391.000000, reward total was -21.000000. running mean: -20.204732\n",
      "episode 392.000000, reward total was -21.000000. running mean: -20.212685\n",
      "episode 393.000000, reward total was -19.000000. running mean: -20.200558\n",
      "episode 394.000000, reward total was -19.000000. running mean: -20.188552\n",
      "episode 395.000000, reward total was -21.000000. running mean: -20.196667\n",
      "episode 396.000000, reward total was -21.000000. running mean: -20.204700\n",
      "episode 397.000000, reward total was -21.000000. running mean: -20.212653\n",
      "episode 398.000000, reward total was -21.000000. running mean: -20.220527\n",
      "episode 399.000000, reward total was -21.000000. running mean: -20.228321\n",
      "episode 400.000000, reward total was -21.000000. running mean: -20.236038\n",
      "episode 401.000000, reward total was -20.000000. running mean: -20.233678\n",
      "episode 402.000000, reward total was -21.000000. running mean: -20.241341\n",
      "episode 403.000000, reward total was -21.000000. running mean: -20.248927\n",
      "episode 404.000000, reward total was -19.000000. running mean: -20.236438\n",
      "episode 405.000000, reward total was -19.000000. running mean: -20.224074\n",
      "episode 406.000000, reward total was -21.000000. running mean: -20.231833\n",
      "episode 407.000000, reward total was -20.000000. running mean: -20.229515\n",
      "episode 408.000000, reward total was -21.000000. running mean: -20.237220\n",
      "episode 409.000000, reward total was -21.000000. running mean: -20.244847\n",
      "episode 410.000000, reward total was -20.000000. running mean: -20.242399\n",
      "episode 411.000000, reward total was -21.000000. running mean: -20.249975\n",
      "episode 412.000000, reward total was -21.000000. running mean: -20.257475\n",
      "episode 413.000000, reward total was -21.000000. running mean: -20.264900\n",
      "episode 414.000000, reward total was -18.000000. running mean: -20.242251\n",
      "episode 415.000000, reward total was -20.000000. running mean: -20.239829\n",
      "episode 416.000000, reward total was -19.000000. running mean: -20.227431\n",
      "episode 417.000000, reward total was -20.000000. running mean: -20.225156\n",
      "episode 418.000000, reward total was -20.000000. running mean: -20.222905\n",
      "episode 419.000000, reward total was -21.000000. running mean: -20.230676\n",
      "episode 420.000000, reward total was -21.000000. running mean: -20.238369\n",
      "episode 421.000000, reward total was -17.000000. running mean: -20.205985\n",
      "episode 422.000000, reward total was -20.000000. running mean: -20.203925\n",
      "episode 423.000000, reward total was -21.000000. running mean: -20.211886\n",
      "episode 424.000000, reward total was -20.000000. running mean: -20.209767\n",
      "episode 425.000000, reward total was -20.000000. running mean: -20.207670\n",
      "episode 426.000000, reward total was -20.000000. running mean: -20.205593\n",
      "episode 427.000000, reward total was -21.000000. running mean: -20.213537\n",
      "episode 428.000000, reward total was -21.000000. running mean: -20.221402\n",
      "episode 429.000000, reward total was -21.000000. running mean: -20.229188\n",
      "episode 430.000000, reward total was -20.000000. running mean: -20.226896\n",
      "episode 431.000000, reward total was -21.000000. running mean: -20.234627\n",
      "episode 432.000000, reward total was -21.000000. running mean: -20.242281\n",
      "episode 433.000000, reward total was -20.000000. running mean: -20.239858\n",
      "episode 434.000000, reward total was -21.000000. running mean: -20.247459\n",
      "episode 435.000000, reward total was -21.000000. running mean: -20.254985\n",
      "episode 436.000000, reward total was -20.000000. running mean: -20.252435\n",
      "episode 437.000000, reward total was -21.000000. running mean: -20.259910\n",
      "episode 438.000000, reward total was -20.000000. running mean: -20.257311\n",
      "episode 439.000000, reward total was -21.000000. running mean: -20.264738\n",
      "episode 440.000000, reward total was -19.000000. running mean: -20.252091\n",
      "episode 441.000000, reward total was -21.000000. running mean: -20.259570\n",
      "episode 442.000000, reward total was -20.000000. running mean: -20.256974\n",
      "episode 443.000000, reward total was -21.000000. running mean: -20.264404\n",
      "episode 444.000000, reward total was -21.000000. running mean: -20.271760\n",
      "episode 445.000000, reward total was -21.000000. running mean: -20.279043\n",
      "episode 446.000000, reward total was -21.000000. running mean: -20.286252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 447.000000, reward total was -21.000000. running mean: -20.293390\n",
      "episode 448.000000, reward total was -21.000000. running mean: -20.300456\n",
      "episode 449.000000, reward total was -20.000000. running mean: -20.297451\n",
      "episode 450.000000, reward total was -21.000000. running mean: -20.304477\n",
      "episode 451.000000, reward total was -21.000000. running mean: -20.311432\n",
      "episode 452.000000, reward total was -21.000000. running mean: -20.318318\n",
      "episode 453.000000, reward total was -21.000000. running mean: -20.325135\n",
      "episode 454.000000, reward total was -20.000000. running mean: -20.321883\n",
      "episode 455.000000, reward total was -20.000000. running mean: -20.318664\n",
      "episode 456.000000, reward total was -21.000000. running mean: -20.325478\n",
      "episode 457.000000, reward total was -20.000000. running mean: -20.322223\n",
      "episode 458.000000, reward total was -21.000000. running mean: -20.329001\n",
      "episode 459.000000, reward total was -21.000000. running mean: -20.335711\n",
      "episode 460.000000, reward total was -18.000000. running mean: -20.312354\n",
      "episode 461.000000, reward total was -21.000000. running mean: -20.319230\n",
      "episode 462.000000, reward total was -21.000000. running mean: -20.326038\n",
      "episode 463.000000, reward total was -21.000000. running mean: -20.332777\n",
      "episode 464.000000, reward total was -20.000000. running mean: -20.329450\n",
      "episode 465.000000, reward total was -20.000000. running mean: -20.326155\n",
      "episode 466.000000, reward total was -21.000000. running mean: -20.332894\n",
      "episode 467.000000, reward total was -21.000000. running mean: -20.339565\n",
      "episode 468.000000, reward total was -20.000000. running mean: -20.336169\n",
      "episode 469.000000, reward total was -21.000000. running mean: -20.342807\n",
      "episode 470.000000, reward total was -21.000000. running mean: -20.349379\n",
      "episode 471.000000, reward total was -20.000000. running mean: -20.345885\n",
      "episode 472.000000, reward total was -21.000000. running mean: -20.352427\n",
      "episode 473.000000, reward total was -21.000000. running mean: -20.358902\n",
      "episode 474.000000, reward total was -21.000000. running mean: -20.365313\n",
      "episode 475.000000, reward total was -21.000000. running mean: -20.371660\n",
      "episode 476.000000, reward total was -21.000000. running mean: -20.377944\n",
      "episode 477.000000, reward total was -20.000000. running mean: -20.374164\n",
      "episode 478.000000, reward total was -21.000000. running mean: -20.380423\n",
      "episode 479.000000, reward total was -21.000000. running mean: -20.386618\n",
      "episode 480.000000, reward total was -21.000000. running mean: -20.392752\n",
      "episode 481.000000, reward total was -19.000000. running mean: -20.378825\n",
      "episode 482.000000, reward total was -21.000000. running mean: -20.385036\n",
      "episode 483.000000, reward total was -21.000000. running mean: -20.391186\n",
      "episode 484.000000, reward total was -20.000000. running mean: -20.387274\n",
      "episode 485.000000, reward total was -20.000000. running mean: -20.383401\n",
      "episode 486.000000, reward total was -19.000000. running mean: -20.369567\n",
      "episode 487.000000, reward total was -21.000000. running mean: -20.375872\n",
      "episode 488.000000, reward total was -19.000000. running mean: -20.362113\n",
      "episode 489.000000, reward total was -20.000000. running mean: -20.358492\n",
      "episode 490.000000, reward total was -21.000000. running mean: -20.364907\n",
      "episode 491.000000, reward total was -21.000000. running mean: -20.371258\n",
      "episode 492.000000, reward total was -19.000000. running mean: -20.357545\n",
      "episode 493.000000, reward total was -19.000000. running mean: -20.343970\n",
      "episode 494.000000, reward total was -21.000000. running mean: -20.350530\n",
      "episode 495.000000, reward total was -19.000000. running mean: -20.337025\n",
      "episode 496.000000, reward total was -21.000000. running mean: -20.343655\n",
      "episode 497.000000, reward total was -20.000000. running mean: -20.340218\n",
      "episode 498.000000, reward total was -20.000000. running mean: -20.336816\n",
      "episode 499.000000, reward total was -19.000000. running mean: -20.323448\n",
      "episode 500.000000, reward total was -21.000000. running mean: -20.330213\n",
      "episode 501.000000, reward total was -21.000000. running mean: -20.336911\n",
      "episode 502.000000, reward total was -20.000000. running mean: -20.333542\n",
      "episode 503.000000, reward total was -20.000000. running mean: -20.330207\n",
      "episode 504.000000, reward total was -21.000000. running mean: -20.336904\n",
      "episode 505.000000, reward total was -21.000000. running mean: -20.343535\n",
      "episode 506.000000, reward total was -21.000000. running mean: -20.350100\n",
      "episode 507.000000, reward total was -18.000000. running mean: -20.326599\n",
      "episode 508.000000, reward total was -19.000000. running mean: -20.313333\n",
      "episode 509.000000, reward total was -20.000000. running mean: -20.310200\n",
      "episode 510.000000, reward total was -21.000000. running mean: -20.317098\n",
      "episode 511.000000, reward total was -21.000000. running mean: -20.323927\n",
      "episode 512.000000, reward total was -20.000000. running mean: -20.320688\n",
      "episode 513.000000, reward total was -20.000000. running mean: -20.317481\n",
      "episode 514.000000, reward total was -21.000000. running mean: -20.324306\n",
      "episode 515.000000, reward total was -17.000000. running mean: -20.291063\n",
      "episode 516.000000, reward total was -21.000000. running mean: -20.298152\n",
      "episode 517.000000, reward total was -21.000000. running mean: -20.305171\n",
      "episode 518.000000, reward total was -20.000000. running mean: -20.302119\n",
      "episode 519.000000, reward total was -21.000000. running mean: -20.309098\n",
      "episode 520.000000, reward total was -20.000000. running mean: -20.306007\n",
      "episode 521.000000, reward total was -21.000000. running mean: -20.312947\n",
      "episode 522.000000, reward total was -21.000000. running mean: -20.319817\n",
      "episode 523.000000, reward total was -21.000000. running mean: -20.326619\n",
      "episode 524.000000, reward total was -21.000000. running mean: -20.333353\n",
      "episode 525.000000, reward total was -21.000000. running mean: -20.340019\n",
      "episode 526.000000, reward total was -19.000000. running mean: -20.326619\n",
      "episode 527.000000, reward total was -21.000000. running mean: -20.333353\n",
      "episode 528.000000, reward total was -21.000000. running mean: -20.340019\n",
      "episode 529.000000, reward total was -21.000000. running mean: -20.346619\n",
      "episode 530.000000, reward total was -20.000000. running mean: -20.343153\n",
      "episode 531.000000, reward total was -19.000000. running mean: -20.329721\n",
      "episode 532.000000, reward total was -21.000000. running mean: -20.336424\n",
      "episode 533.000000, reward total was -21.000000. running mean: -20.343060\n",
      "episode 534.000000, reward total was -21.000000. running mean: -20.349629\n",
      "episode 535.000000, reward total was -20.000000. running mean: -20.346133\n",
      "episode 536.000000, reward total was -20.000000. running mean: -20.342672\n",
      "episode 537.000000, reward total was -20.000000. running mean: -20.339245\n",
      "episode 538.000000, reward total was -19.000000. running mean: -20.325853\n",
      "episode 539.000000, reward total was -20.000000. running mean: -20.322594\n",
      "episode 540.000000, reward total was -21.000000. running mean: -20.329368\n",
      "episode 541.000000, reward total was -20.000000. running mean: -20.326074\n",
      "episode 542.000000, reward total was -20.000000. running mean: -20.322814\n",
      "episode 543.000000, reward total was -20.000000. running mean: -20.319586\n",
      "episode 544.000000, reward total was -21.000000. running mean: -20.326390\n",
      "episode 545.000000, reward total was -21.000000. running mean: -20.333126\n",
      "episode 546.000000, reward total was -20.000000. running mean: -20.329795\n",
      "episode 547.000000, reward total was -19.000000. running mean: -20.316497\n",
      "episode 548.000000, reward total was -21.000000. running mean: -20.323332\n",
      "episode 549.000000, reward total was -20.000000. running mean: -20.320098\n",
      "episode 550.000000, reward total was -17.000000. running mean: -20.286897\n",
      "episode 551.000000, reward total was -19.000000. running mean: -20.274028\n",
      "episode 552.000000, reward total was -19.000000. running mean: -20.261288\n",
      "episode 553.000000, reward total was -21.000000. running mean: -20.268675\n",
      "episode 554.000000, reward total was -20.000000. running mean: -20.265988\n",
      "episode 555.000000, reward total was -21.000000. running mean: -20.273329\n",
      "episode 556.000000, reward total was -20.000000. running mean: -20.270595\n",
      "episode 557.000000, reward total was -21.000000. running mean: -20.277889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 558.000000, reward total was -20.000000. running mean: -20.275110\n",
      "episode 559.000000, reward total was -21.000000. running mean: -20.282359\n"
     ]
    }
   ],
   "source": [
    "%time hist1 = train_model(env, model, total_episodes=5500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413e6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c18ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f31fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
